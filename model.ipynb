{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "795a375c",
   "metadata": {},
   "source": [
    "# Alter Ego Chess Engine\n",
    "\n",
    "The purpose of the project is to create a simple chess engine trained using Neural Networks. The benchmark for the engine is an ability to win 4 out of 5 games against my friends (~1000-1500 elo)\n",
    "\n",
    "Dataset -> https://www.kaggle.com/datasets/mysarahmadbhat/online-chess-games"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6c6cea1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8c03bb02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>game_id</th>\n",
       "      <th>rated</th>\n",
       "      <th>turns</th>\n",
       "      <th>victory_status</th>\n",
       "      <th>winner</th>\n",
       "      <th>time_increment</th>\n",
       "      <th>white_id</th>\n",
       "      <th>white_rating</th>\n",
       "      <th>black_id</th>\n",
       "      <th>black_rating</th>\n",
       "      <th>moves</th>\n",
       "      <th>opening_code</th>\n",
       "      <th>opening_moves</th>\n",
       "      <th>opening_fullname</th>\n",
       "      <th>opening_shortname</th>\n",
       "      <th>opening_response</th>\n",
       "      <th>opening_variation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>20053</th>\n",
       "      <td>20054</td>\n",
       "      <td>True</td>\n",
       "      <td>24</td>\n",
       "      <td>Resign</td>\n",
       "      <td>White</td>\n",
       "      <td>10+10</td>\n",
       "      <td>belcolt</td>\n",
       "      <td>1691</td>\n",
       "      <td>jamboger</td>\n",
       "      <td>1220</td>\n",
       "      <td>d4 f5 e3 e6 Nf3 Nf6 Nc3 b6 Be2 Bb7 O-O Be7 Ne5...</td>\n",
       "      <td>A80</td>\n",
       "      <td>2</td>\n",
       "      <td>Dutch Defense</td>\n",
       "      <td>Dutch Defense</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20054</th>\n",
       "      <td>20055</td>\n",
       "      <td>True</td>\n",
       "      <td>82</td>\n",
       "      <td>Mate</td>\n",
       "      <td>Black</td>\n",
       "      <td>10+0</td>\n",
       "      <td>jamboger</td>\n",
       "      <td>1233</td>\n",
       "      <td>farrukhasomiddinov</td>\n",
       "      <td>1196</td>\n",
       "      <td>d4 d6 Bf4 e5 Bg3 Nf6 e3 exd4 exd4 d5 c3 Bd6 Bd...</td>\n",
       "      <td>A41</td>\n",
       "      <td>2</td>\n",
       "      <td>Queen's Pawn</td>\n",
       "      <td>Queen's Pawn</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20055</th>\n",
       "      <td>20056</td>\n",
       "      <td>True</td>\n",
       "      <td>35</td>\n",
       "      <td>Mate</td>\n",
       "      <td>White</td>\n",
       "      <td>10+0</td>\n",
       "      <td>jamboger</td>\n",
       "      <td>1219</td>\n",
       "      <td>schaaksmurf3</td>\n",
       "      <td>1286</td>\n",
       "      <td>d4 d5 Bf4 Nc6 e3 Nf6 c3 e6 Nf3 Be7 Bd3 O-O Nbd...</td>\n",
       "      <td>D00</td>\n",
       "      <td>3</td>\n",
       "      <td>Queen's Pawn Game: Mason Attack</td>\n",
       "      <td>Queen's Pawn Game</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Mason Attack</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20056</th>\n",
       "      <td>20057</td>\n",
       "      <td>True</td>\n",
       "      <td>109</td>\n",
       "      <td>Resign</td>\n",
       "      <td>White</td>\n",
       "      <td>10+0</td>\n",
       "      <td>marcodisogno</td>\n",
       "      <td>1360</td>\n",
       "      <td>jamboger</td>\n",
       "      <td>1227</td>\n",
       "      <td>e4 d6 d4 Nf6 e5 dxe5 dxe5 Qxd1+ Kxd1 Nd5 c4 Nb...</td>\n",
       "      <td>B07</td>\n",
       "      <td>4</td>\n",
       "      <td>Pirc Defense</td>\n",
       "      <td>Pirc Defense</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20057</th>\n",
       "      <td>20058</td>\n",
       "      <td>True</td>\n",
       "      <td>78</td>\n",
       "      <td>Mate</td>\n",
       "      <td>Black</td>\n",
       "      <td>10+0</td>\n",
       "      <td>jamboger</td>\n",
       "      <td>1235</td>\n",
       "      <td>ffbob</td>\n",
       "      <td>1339</td>\n",
       "      <td>d4 d5 Bf4 Na6 e3 e6 c3 Nf6 Nf3 Bd7 Nbd2 b5 Bd3...</td>\n",
       "      <td>D00</td>\n",
       "      <td>3</td>\n",
       "      <td>Queen's Pawn Game: Mason Attack</td>\n",
       "      <td>Queen's Pawn Game</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Mason Attack</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       game_id  rated  turns victory_status winner time_increment  \\\n",
       "20053    20054   True     24         Resign  White          10+10   \n",
       "20054    20055   True     82           Mate  Black           10+0   \n",
       "20055    20056   True     35           Mate  White           10+0   \n",
       "20056    20057   True    109         Resign  White           10+0   \n",
       "20057    20058   True     78           Mate  Black           10+0   \n",
       "\n",
       "           white_id  white_rating            black_id  black_rating  \\\n",
       "20053       belcolt          1691            jamboger          1220   \n",
       "20054      jamboger          1233  farrukhasomiddinov          1196   \n",
       "20055      jamboger          1219        schaaksmurf3          1286   \n",
       "20056  marcodisogno          1360            jamboger          1227   \n",
       "20057      jamboger          1235               ffbob          1339   \n",
       "\n",
       "                                                   moves opening_code  \\\n",
       "20053  d4 f5 e3 e6 Nf3 Nf6 Nc3 b6 Be2 Bb7 O-O Be7 Ne5...          A80   \n",
       "20054  d4 d6 Bf4 e5 Bg3 Nf6 e3 exd4 exd4 d5 c3 Bd6 Bd...          A41   \n",
       "20055  d4 d5 Bf4 Nc6 e3 Nf6 c3 e6 Nf3 Be7 Bd3 O-O Nbd...          D00   \n",
       "20056  e4 d6 d4 Nf6 e5 dxe5 dxe5 Qxd1+ Kxd1 Nd5 c4 Nb...          B07   \n",
       "20057  d4 d5 Bf4 Na6 e3 e6 c3 Nf6 Nf3 Bd7 Nbd2 b5 Bd3...          D00   \n",
       "\n",
       "       opening_moves                 opening_fullname  opening_shortname  \\\n",
       "20053              2                    Dutch Defense      Dutch Defense   \n",
       "20054              2                     Queen's Pawn       Queen's Pawn   \n",
       "20055              3  Queen's Pawn Game: Mason Attack  Queen's Pawn Game   \n",
       "20056              4                     Pirc Defense       Pirc Defense   \n",
       "20057              3  Queen's Pawn Game: Mason Attack  Queen's Pawn Game   \n",
       "\n",
       "      opening_response opening_variation  \n",
       "20053              NaN               NaN  \n",
       "20054              NaN               NaN  \n",
       "20055              NaN      Mason Attack  \n",
       "20056              NaN               NaN  \n",
       "20057              NaN      Mason Attack  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_df = pd.read_csv('./data/chess_games.csv')\n",
    "data_df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d5d247f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 249 games with rating <= 1000 out of total games 20058\n",
      "There are 6325 games that ended up with mates out of total games 20058\n",
      "There are 16715 games have more than 30 turns out of total games 20058\n"
     ]
    }
   ],
   "source": [
    "white_rating = data_df['white_rating']\n",
    "victory_status = data_df['victory_status']\n",
    "turns = data_df['turns']\n",
    "\n",
    "print(f\"There are {white_rating.where(white_rating <= 1000).count()} games with rating <= 1000 out of total games {len(data_df)}\")\n",
    "print(f\"There are {victory_status.where(victory_status == 'Mate').count()} games that ended up with mates out of total games {len(data_df)}\")\n",
    "print(f\"There are {turns.where(turns >= 30).count()} games have more than 30 turns out of total games {len(data_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "742b0385",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['e4',\n",
       " 'e5',\n",
       " 'd3',\n",
       " 'd6',\n",
       " 'Be3',\n",
       " 'c6',\n",
       " 'Be2',\n",
       " 'b5',\n",
       " 'Nd2',\n",
       " 'a5',\n",
       " 'a4',\n",
       " 'c5',\n",
       " 'axb5',\n",
       " 'Nc6',\n",
       " 'bxc6',\n",
       " 'Ra6',\n",
       " 'Nc4',\n",
       " 'a4',\n",
       " 'c3',\n",
       " 'a3',\n",
       " 'Nxa3',\n",
       " 'Rxa3',\n",
       " 'Rxa3',\n",
       " 'c4',\n",
       " 'dxc4',\n",
       " 'd5',\n",
       " 'cxd5',\n",
       " 'Qxd5',\n",
       " 'exd5',\n",
       " 'Be6',\n",
       " 'Ra8+',\n",
       " 'Ke7',\n",
       " 'Bc5+',\n",
       " 'Kf6',\n",
       " 'Bxf8',\n",
       " 'Kg6',\n",
       " 'Bxg7',\n",
       " 'Kxg7',\n",
       " 'dxe6',\n",
       " 'Kh6',\n",
       " 'exf7',\n",
       " 'Nf6',\n",
       " 'Rxh8',\n",
       " 'Nh5',\n",
       " 'Bxh5',\n",
       " 'Kg5',\n",
       " 'Rxh7',\n",
       " 'Kf5',\n",
       " 'Qf3+',\n",
       " 'Ke6',\n",
       " 'Bg4+',\n",
       " 'Kd6',\n",
       " 'Rh6+',\n",
       " 'Kc5',\n",
       " 'Qe3+',\n",
       " 'Kb5',\n",
       " 'c4+',\n",
       " 'Kb4',\n",
       " 'Qc3+',\n",
       " 'Ka4',\n",
       " 'Bd1#']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_game = data_df['moves'][2]\n",
    "test_game_moves = test_game.split(' ')\n",
    "test_game_moves"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54c0c98c",
   "metadata": {},
   "source": [
    "Let's try to think a little bit about the neural network. Basically a chess board is a 8x8 matrix, we can use it as input right away. As for the output, for now we want to simply predict next move, without doing any \"fancy\" SOTA techniques. We can represent output as probabilities of \"from_square\" & \"to_square\" encoded in 4096 vector\n",
    "\n",
    "The encoded version is calculated by:\n",
    "\n",
    "$$position_{from} * 63 + position_{to}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "92b665a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" viewBox=\"0 0 390 390\" width=\"350\" height=\"350\"><desc><pre>. . . . . . . .\n",
       ". . . . . P . .\n",
       ". . P . . . . R\n",
       ". . . . p . . .\n",
       "k . P . . . . .\n",
       ". . Q . . . . .\n",
       ". P . . . P P P\n",
       ". . . B K . N R</pre></desc><defs><g id=\"white-pawn\" class=\"white pawn\"><path d=\"M22.5 9c-2.21 0-4 1.79-4 4 0 .89.29 1.71.78 2.38C17.33 16.5 16 18.59 16 21c0 2.03.94 3.84 2.41 5.03-3 1.06-7.41 5.55-7.41 13.47h23c0-7.92-4.41-12.41-7.41-13.47 1.47-1.19 2.41-3 2.41-5.03 0-2.41-1.33-4.5-3.28-5.62.49-.67.78-1.49.78-2.38 0-2.21-1.79-4-4-4z\" fill=\"#fff\" stroke=\"#000\" stroke-width=\"1.5\" stroke-linecap=\"round\"/></g><g id=\"white-knight\" class=\"white knight\" fill=\"none\" fill-rule=\"evenodd\" stroke=\"#000\" stroke-width=\"1.5\" stroke-linecap=\"round\" stroke-linejoin=\"round\"><path d=\"M 22,10 C 32.5,11 38.5,18 38,39 L 15,39 C 15,30 25,32.5 23,18\" style=\"fill:#ffffff; stroke:#000000;\"/><path d=\"M 24,18 C 24.38,20.91 18.45,25.37 16,27 C 13,29 13.18,31.34 11,31 C 9.958,30.06 12.41,27.96 11,28 C 10,28 11.19,29.23 10,30 C 9,30 5.997,31 6,26 C 6,24 12,14 12,14 C 12,14 13.89,12.1 14,10.5 C 13.27,9.506 13.5,8.5 13.5,7.5 C 14.5,6.5 16.5,10 16.5,10 L 18.5,10 C 18.5,10 19.28,8.008 21,7 C 22,7 22,10 22,10\" style=\"fill:#ffffff; stroke:#000000;\"/><path d=\"M 9.5 25.5 A 0.5 0.5 0 1 1 8.5,25.5 A 0.5 0.5 0 1 1 9.5 25.5 z\" style=\"fill:#000000; stroke:#000000;\"/><path d=\"M 15 15.5 A 0.5 1.5 0 1 1 14,15.5 A 0.5 1.5 0 1 1 15 15.5 z\" transform=\"matrix(0.866,0.5,-0.5,0.866,9.693,-5.173)\" style=\"fill:#000000; stroke:#000000;\"/></g><g id=\"white-bishop\" class=\"white bishop\" fill=\"none\" fill-rule=\"evenodd\" stroke=\"#000\" stroke-width=\"1.5\" stroke-linecap=\"round\" stroke-linejoin=\"round\"><g fill=\"#fff\" stroke-linecap=\"butt\"><path d=\"M9 36c3.39-.97 10.11.43 13.5-2 3.39 2.43 10.11 1.03 13.5 2 0 0 1.65.54 3 2-.68.97-1.65.99-3 .5-3.39-.97-10.11.46-13.5-1-3.39 1.46-10.11.03-13.5 1-1.354.49-2.323.47-3-.5 1.354-1.94 3-2 3-2zM15 32c2.5 2.5 12.5 2.5 15 0 .5-1.5 0-2 0-2 0-2.5-2.5-4-2.5-4 5.5-1.5 6-11.5-5-15.5-11 4-10.5 14-5 15.5 0 0-2.5 1.5-2.5 4 0 0-.5.5 0 2zM25 8a2.5 2.5 0 1 1-5 0 2.5 2.5 0 1 1 5 0z\"/></g><path d=\"M17.5 26h10M15 30h15m-7.5-14.5v5M20 18h5\" stroke-linejoin=\"miter\"/></g><g id=\"white-rook\" class=\"white rook\" fill=\"#fff\" fill-rule=\"evenodd\" stroke=\"#000\" stroke-width=\"1.5\" stroke-linecap=\"round\" stroke-linejoin=\"round\"><path d=\"M9 39h27v-3H9v3zM12 36v-4h21v4H12zM11 14V9h4v2h5V9h5v2h5V9h4v5\" stroke-linecap=\"butt\"/><path d=\"M34 14l-3 3H14l-3-3\"/><path d=\"M31 17v12.5H14V17\" stroke-linecap=\"butt\" stroke-linejoin=\"miter\"/><path d=\"M31 29.5l1.5 2.5h-20l1.5-2.5\"/><path d=\"M11 14h23\" fill=\"none\" stroke-linejoin=\"miter\"/></g><g id=\"white-queen\" class=\"white queen\" fill=\"#fff\" fill-rule=\"evenodd\" stroke=\"#000\" stroke-width=\"1.5\" stroke-linecap=\"round\" stroke-linejoin=\"round\"><path d=\"M8 12a2 2 0 1 1-4 0 2 2 0 1 1 4 0zM24.5 7.5a2 2 0 1 1-4 0 2 2 0 1 1 4 0zM41 12a2 2 0 1 1-4 0 2 2 0 1 1 4 0zM16 8.5a2 2 0 1 1-4 0 2 2 0 1 1 4 0zM33 9a2 2 0 1 1-4 0 2 2 0 1 1 4 0z\"/><path d=\"M9 26c8.5-1.5 21-1.5 27 0l2-12-7 11V11l-5.5 13.5-3-15-3 15-5.5-14V25L7 14l2 12zM9 26c0 2 1.5 2 2.5 4 1 1.5 1 1 .5 3.5-1.5 1-1.5 2.5-1.5 2.5-1.5 1.5.5 2.5.5 2.5 6.5 1 16.5 1 23 0 0 0 1.5-1 0-2.5 0 0 .5-1.5-1-2.5-.5-2.5-.5-2 .5-3.5 1-2 2.5-2 2.5-4-8.5-1.5-18.5-1.5-27 0z\" stroke-linecap=\"butt\"/><path d=\"M11.5 30c3.5-1 18.5-1 22 0M12 33.5c6-1 15-1 21 0\" fill=\"none\"/></g><g id=\"white-king\" class=\"white king\" fill=\"none\" fill-rule=\"evenodd\" stroke=\"#000\" stroke-width=\"1.5\" stroke-linecap=\"round\" stroke-linejoin=\"round\"><path d=\"M22.5 11.63V6M20 8h5\" stroke-linejoin=\"miter\"/><path d=\"M22.5 25s4.5-7.5 3-10.5c0 0-1-2.5-3-2.5s-3 2.5-3 2.5c-1.5 3 3 10.5 3 10.5\" fill=\"#fff\" stroke-linecap=\"butt\" stroke-linejoin=\"miter\"/><path d=\"M11.5 37c5.5 3.5 15.5 3.5 21 0v-7s9-4.5 6-10.5c-4-6.5-13.5-3.5-16 4V27v-3.5c-3.5-7.5-13-10.5-16-4-3 6 5 10 5 10V37z\" fill=\"#fff\"/><path d=\"M11.5 30c5.5-3 15.5-3 21 0m-21 3.5c5.5-3 15.5-3 21 0m-21 3.5c5.5-3 15.5-3 21 0\"/></g><g id=\"black-pawn\" class=\"black pawn\"><path d=\"M22.5 9c-2.21 0-4 1.79-4 4 0 .89.29 1.71.78 2.38C17.33 16.5 16 18.59 16 21c0 2.03.94 3.84 2.41 5.03-3 1.06-7.41 5.55-7.41 13.47h23c0-7.92-4.41-12.41-7.41-13.47 1.47-1.19 2.41-3 2.41-5.03 0-2.41-1.33-4.5-3.28-5.62.49-.67.78-1.49.78-2.38 0-2.21-1.79-4-4-4z\" fill=\"#000\" stroke=\"#000\" stroke-width=\"1.5\" stroke-linecap=\"round\"/></g><g id=\"black-king\" class=\"black king\" fill=\"none\" fill-rule=\"evenodd\" stroke=\"#000\" stroke-width=\"1.5\" stroke-linecap=\"round\" stroke-linejoin=\"round\"><path d=\"M22.5 11.63V6\" stroke-linejoin=\"miter\"/><path d=\"M22.5 25s4.5-7.5 3-10.5c0 0-1-2.5-3-2.5s-3 2.5-3 2.5c-1.5 3 3 10.5 3 10.5\" fill=\"#000\" stroke-linecap=\"butt\" stroke-linejoin=\"miter\"/><path d=\"M11.5 37c5.5 3.5 15.5 3.5 21 0v-7s9-4.5 6-10.5c-4-6.5-13.5-3.5-16 4V27v-3.5c-3.5-7.5-13-10.5-16-4-3 6 5 10 5 10V37z\" fill=\"#000\"/><path d=\"M20 8h5\" stroke-linejoin=\"miter\"/><path d=\"M32 29.5s8.5-4 6.03-9.65C34.15 14 25 18 22.5 24.5l.01 2.1-.01-2.1C20 18 9.906 14 6.997 19.85c-2.497 5.65 4.853 9 4.853 9M11.5 30c5.5-3 15.5-3 21 0m-21 3.5c5.5-3 15.5-3 21 0m-21 3.5c5.5-3 15.5-3 21 0\" stroke=\"#fff\"/></g></defs><rect x=\"7.5\" y=\"7.5\" width=\"375\" height=\"375\" fill=\"none\" stroke=\"#212121\" stroke-width=\"15\"/><g transform=\"translate(20, 1) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M23.328 10.016q-1.742 0-2.414.398-.672.398-.672 1.36 0 .765.5 1.218.508.445 1.375.445 1.196 0 1.914-.843.727-.852.727-2.258v-.32zm2.867-.594v4.992h-1.437v-1.328q-.492.797-1.227 1.18-.734.375-1.797.375-1.343 0-2.14-.75-.79-.758-.79-2.024 0-1.476.985-2.226.992-.75 2.953-.75h2.016V8.75q0-.992-.656-1.531-.649-.547-1.829-.547-.75 0-1.46.18-.711.18-1.368.539V6.062q.79-.304 1.532-.453.742-.156 1.445-.156 1.898 0 2.836.984.937.985.937 2.985z\"/></g><g transform=\"translate(20, 375) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M23.328 10.016q-1.742 0-2.414.398-.672.398-.672 1.36 0 .765.5 1.218.508.445 1.375.445 1.196 0 1.914-.843.727-.852.727-2.258v-.32zm2.867-.594v4.992h-1.437v-1.328q-.492.797-1.227 1.18-.734.375-1.797.375-1.343 0-2.14-.75-.79-.758-.79-2.024 0-1.476.985-2.226.992-.75 2.953-.75h2.016V8.75q0-.992-.656-1.531-.649-.547-1.829-.547-.75 0-1.46.18-.711.18-1.368.539V6.062q.79-.304 1.532-.453.742-.156 1.445-.156 1.898 0 2.836.984.937.985.937 2.985z\"/></g><g transform=\"translate(65, 1) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M24.922 10.047q0-1.586-.656-2.485-.649-.906-1.79-.906-1.14 0-1.796.906-.649.899-.649 2.485 0 1.586.649 2.492.656.898 1.797.898 1.14 0 1.789-.898.656-.906.656-2.492zm-4.89-3.055q.452-.781 1.14-1.156.695-.383 1.656-.383 1.594 0 2.586 1.266 1 1.265 1 3.328 0 2.062-1 3.328-.992 1.266-2.586 1.266-.96 0-1.656-.375-.688-.383-1.14-1.164v1.312h-1.446V2.258h1.445z\"/></g><g transform=\"translate(65, 375) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M24.922 10.047q0-1.586-.656-2.485-.649-.906-1.79-.906-1.14 0-1.796.906-.649.899-.649 2.485 0 1.586.649 2.492.656.898 1.797.898 1.14 0 1.789-.898.656-.906.656-2.492zm-4.89-3.055q.452-.781 1.14-1.156.695-.383 1.656-.383 1.594 0 2.586 1.266 1 1.265 1 3.328 0 2.062-1 3.328-.992 1.266-2.586 1.266-.96 0-1.656-.375-.688-.383-1.14-1.164v1.312h-1.446V2.258h1.445z\"/></g><g transform=\"translate(110, 1) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M25.96 6v1.344q-.608-.336-1.226-.5-.609-.172-1.234-.172-1.398 0-2.172.89-.773.883-.773 2.485 0 1.601.773 2.492.774.883 2.172.883.625 0 1.234-.164.618-.172 1.227-.508v1.328q-.602.281-1.25.422-.64.14-1.367.14-1.977 0-3.14-1.242-1.165-1.242-1.165-3.351 0-2.14 1.172-3.367 1.18-1.227 3.227-1.227.664 0 1.296.14.633.134 1.227.407z\"/></g><g transform=\"translate(110, 375) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M25.96 6v1.344q-.608-.336-1.226-.5-.609-.172-1.234-.172-1.398 0-2.172.89-.773.883-.773 2.485 0 1.601.773 2.492.774.883 2.172.883.625 0 1.234-.164.618-.172 1.227-.508v1.328q-.602.281-1.25.422-.64.14-1.367.14-1.977 0-3.14-1.242-1.165-1.242-1.165-3.351 0-2.14 1.172-3.367 1.18-1.227 3.227-1.227.664 0 1.296.14.633.134 1.227.407z\"/></g><g transform=\"translate(155, 1) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M24.973 6.992V2.258h1.437v12.156h-1.437v-1.312q-.453.78-1.149 1.164-.687.375-1.656.375-1.586 0-2.586-1.266-.992-1.266-.992-3.328 0-2.063.992-3.328 1-1.266 2.586-1.266.969 0 1.656.383.696.375 1.149 1.156zm-4.899 3.055q0 1.586.649 2.492.656.898 1.797.898 1.14 0 1.796-.898.657-.906.657-2.492 0-1.586-.657-2.485-.656-.906-1.796-.906-1.141 0-1.797.906-.649.899-.649 2.485z\"/></g><g transform=\"translate(155, 375) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M24.973 6.992V2.258h1.437v12.156h-1.437v-1.312q-.453.78-1.149 1.164-.687.375-1.656.375-1.586 0-2.586-1.266-.992-1.266-.992-3.328 0-2.063.992-3.328 1-1.266 2.586-1.266.969 0 1.656.383.696.375 1.149 1.156zm-4.899 3.055q0 1.586.649 2.492.656.898 1.797.898 1.14 0 1.796-.898.657-.906.657-2.492 0-1.586-.657-2.485-.656-.906-1.796-.906-1.141 0-1.797.906-.649.899-.649 2.485z\"/></g><g transform=\"translate(200, 1) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M26.555 9.68v.703h-6.61q.094 1.484.89 2.265.806.774 2.235.774.828 0 1.602-.203.781-.203 1.547-.61v1.36q-.774.328-1.586.5-.813.172-1.649.172-2.093 0-3.32-1.22-1.219-1.218-1.219-3.296 0-2.148 1.157-3.406 1.164-1.266 3.132-1.266 1.766 0 2.79 1.14 1.03 1.134 1.03 3.087zm-1.438-.422q-.015-1.18-.664-1.883-.64-.703-1.703-.703-1.203 0-1.93.68-.718.68-.828 1.914z\"/></g><g transform=\"translate(200, 375) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M26.555 9.68v.703h-6.61q.094 1.484.89 2.265.806.774 2.235.774.828 0 1.602-.203.781-.203 1.547-.61v1.36q-.774.328-1.586.5-.813.172-1.649.172-2.093 0-3.32-1.22-1.219-1.218-1.219-3.296 0-2.148 1.157-3.406 1.164-1.266 3.132-1.266 1.766 0 2.79 1.14 1.03 1.134 1.03 3.087zm-1.438-.422q-.015-1.18-.664-1.883-.64-.703-1.703-.703-1.203 0-1.93.68-.718.68-.828 1.914z\"/></g><g transform=\"translate(245, 1) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M25.285 2.258v1.195H23.91q-.773 0-1.078.313-.297.312-.297 1.125v.773h2.367v1.117h-2.367v7.633H21.09V6.781h-1.375V5.664h1.375v-.61q0-1.46.68-2.124.68-.672 2.156-.672z\"/></g><g transform=\"translate(245, 375) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M25.285 2.258v1.195H23.91q-.773 0-1.078.313-.297.312-.297 1.125v.773h2.367v1.117h-2.367v7.633H21.09V6.781h-1.375V5.664h1.375v-.61q0-1.46.68-2.124.68-.672 2.156-.672z\"/></g><g transform=\"translate(290, 1) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M24.973 9.937q0-1.562-.649-2.421-.64-.86-1.804-.86-1.157 0-1.805.86-.64.859-.64 2.421 0 1.555.64 2.415.648.859 1.805.859 1.164 0 1.804-.86.649-.859.649-2.414zm1.437 3.391q0 2.234-.992 3.32-.992 1.094-3.04 1.094-.757 0-1.429-.117-.672-.11-1.304-.344v-1.398q.632.344 1.25.508.617.164 1.257.164 1.414 0 2.118-.743.703-.734.703-2.226v-.711q-.446.773-1.141 1.156-.695.383-1.664.383-1.61 0-2.594-1.227-.984-1.226-.984-3.25 0-2.03.984-3.257.985-1.227 2.594-1.227.969 0 1.664.383t1.14 1.156V5.664h1.438z\"/></g><g transform=\"translate(290, 375) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M24.973 9.937q0-1.562-.649-2.421-.64-.86-1.804-.86-1.157 0-1.805.86-.64.859-.64 2.421 0 1.555.64 2.415.648.859 1.805.859 1.164 0 1.804-.86.649-.859.649-2.414zm1.437 3.391q0 2.234-.992 3.32-.992 1.094-3.04 1.094-.757 0-1.429-.117-.672-.11-1.304-.344v-1.398q.632.344 1.25.508.617.164 1.257.164 1.414 0 2.118-.743.703-.734.703-2.226v-.711q-.446.773-1.141 1.156-.695.383-1.664.383-1.61 0-2.594-1.227-.984-1.226-.984-3.25 0-2.03.984-3.257.985-1.227 2.594-1.227.969 0 1.664.383t1.14 1.156V5.664h1.438z\"/></g><g transform=\"translate(335, 1) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M26.164 9.133v5.281h-1.437V9.18q0-1.243-.485-1.86-.484-.617-1.453-.617-1.164 0-1.836.742-.672.742-.672 2.024v4.945h-1.445V2.258h1.445v4.765q.516-.789 1.211-1.18.703-.39 1.617-.39 1.508 0 2.282.938.773.93.773 2.742z\"/></g><g transform=\"translate(335, 375) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M26.164 9.133v5.281h-1.437V9.18q0-1.243-.485-1.86-.484-.617-1.453-.617-1.164 0-1.836.742-.672.742-.672 2.024v4.945h-1.445V2.258h1.445v4.765q.516-.789 1.211-1.18.703-.39 1.617-.39 1.508 0 2.282.938.773.93.773 2.742z\"/></g><g transform=\"translate(0, 335) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M6.754 26.996h2.578v-8.898l-2.805.562v-1.437l2.79-.563h1.578v10.336h2.578v1.328h-6.72z\"/></g><g transform=\"translate(375, 335) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M6.754 26.996h2.578v-8.898l-2.805.562v-1.437l2.79-.563h1.578v10.336h2.578v1.328h-6.72z\"/></g><g transform=\"translate(0, 290) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M8.195 26.996h5.508v1.328H6.297v-1.328q.898-.93 2.445-2.492 1.555-1.57 1.953-2.024.758-.851 1.055-1.437.305-.594.305-1.164 0-.93-.657-1.516-.648-.586-1.695-.586-.742 0-1.57.258-.82.258-1.758.781v-1.593q.953-.383 1.781-.578.828-.196 1.516-.196 1.812 0 2.89.906 1.079.907 1.079 2.422 0 .72-.274 1.368-.265.64-.976 1.515-.196.227-1.243 1.313-1.046 1.078-2.953 3.023z\"/></g><g transform=\"translate(375, 290) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M8.195 26.996h5.508v1.328H6.297v-1.328q.898-.93 2.445-2.492 1.555-1.57 1.953-2.024.758-.851 1.055-1.437.305-.594.305-1.164 0-.93-.657-1.516-.648-.586-1.695-.586-.742 0-1.57.258-.82.258-1.758.781v-1.593q.953-.383 1.781-.578.828-.196 1.516-.196 1.812 0 2.89.906 1.079.907 1.079 2.422 0 .72-.274 1.368-.265.64-.976 1.515-.196.227-1.243 1.313-1.046 1.078-2.953 3.023z\"/></g><g transform=\"translate(0, 245) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M11.434 22.035q1.132.242 1.765 1.008.64.766.64 1.89 0 1.727-1.187 2.672-1.187.946-3.375.946-.734 0-1.515-.149-.774-.14-1.602-.43V26.45q.656.383 1.438.578.78.196 1.632.196 1.485 0 2.258-.586.782-.586.782-1.703 0-1.032-.727-1.61-.719-.586-2.008-.586h-1.36v-1.297h1.423q1.164 0 1.78-.46.618-.47.618-1.344 0-.899-.64-1.375-.633-.485-1.82-.485-.65 0-1.391.141-.743.14-1.633.437V16.95q.898-.25 1.68-.375.788-.125 1.484-.125 1.797 0 2.844.82 1.046.813 1.046 2.204 0 .968-.554 1.64-.555.664-1.578.922z\"/></g><g transform=\"translate(375, 245) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M11.434 22.035q1.132.242 1.765 1.008.64.766.64 1.89 0 1.727-1.187 2.672-1.187.946-3.375.946-.734 0-1.515-.149-.774-.14-1.602-.43V26.45q.656.383 1.438.578.78.196 1.632.196 1.485 0 2.258-.586.782-.586.782-1.703 0-1.032-.727-1.61-.719-.586-2.008-.586h-1.36v-1.297h1.423q1.164 0 1.78-.46.618-.47.618-1.344 0-.899-.64-1.375-.633-.485-1.82-.485-.65 0-1.391.141-.743.14-1.633.437V16.95q.898-.25 1.68-.375.788-.125 1.484-.125 1.797 0 2.844.82 1.046.813 1.046 2.204 0 .968-.554 1.64-.555.664-1.578.922z\"/></g><g transform=\"translate(0, 200) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M11.016 18.035L7.03 24.262h3.985zm-.414-1.375h1.984v7.602h1.664v1.312h-1.664v2.75h-1.57v-2.75H5.75v-1.523z\"/></g><g transform=\"translate(375, 200) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M11.016 18.035L7.03 24.262h3.985zm-.414-1.375h1.984v7.602h1.664v1.312h-1.664v2.75h-1.57v-2.75H5.75v-1.523z\"/></g><g transform=\"translate(0, 155) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M6.719 16.66h6.195v1.328h-4.75v2.86q.344-.118.688-.172.343-.063.687-.063 1.953 0 3.094 1.07 1.14 1.07 1.14 2.899 0 1.883-1.171 2.93-1.172 1.039-3.305 1.039-.735 0-1.5-.125-.758-.125-1.57-.375v-1.586q.703.383 1.453.57.75.188 1.586.188 1.351 0 2.14-.711.79-.711.79-1.93 0-1.219-.79-1.93-.789-.71-2.14-.71-.633 0-1.266.14-.625.14-1.281.438z\"/></g><g transform=\"translate(375, 155) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M6.719 16.66h6.195v1.328h-4.75v2.86q.344-.118.688-.172.343-.063.687-.063 1.953 0 3.094 1.07 1.14 1.07 1.14 2.899 0 1.883-1.171 2.93-1.172 1.039-3.305 1.039-.735 0-1.5-.125-.758-.125-1.57-.375v-1.586q.703.383 1.453.57.75.188 1.586.188 1.351 0 2.14-.711.79-.711.79-1.93 0-1.219-.79-1.93-.789-.71-2.14-.71-.633 0-1.266.14-.625.14-1.281.438z\"/></g><g transform=\"translate(0, 110) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M10.137 21.863q-1.063 0-1.688.727-.617.726-.617 1.992 0 1.258.617 1.992.625.727 1.688.727 1.062 0 1.68-.727.624-.734.624-1.992 0-1.266-.625-1.992-.617-.727-1.68-.727zm3.133-4.945v1.437q-.594-.28-1.204-.43-.601-.148-1.195-.148-1.562 0-2.39 1.055-.82 1.055-.938 3.188.46-.68 1.156-1.04.696-.367 1.531-.367 1.758 0 2.774 1.07 1.023 1.063 1.023 2.899 0 1.797-1.062 2.883-1.063 1.086-2.828 1.086-2.024 0-3.094-1.547-1.07-1.555-1.07-4.5 0-2.766 1.312-4.406 1.313-1.649 3.524-1.649.593 0 1.195.117.61.118 1.266.352z\"/></g><g transform=\"translate(375, 110) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M10.137 21.863q-1.063 0-1.688.727-.617.726-.617 1.992 0 1.258.617 1.992.625.727 1.688.727 1.062 0 1.68-.727.624-.734.624-1.992 0-1.266-.625-1.992-.617-.727-1.68-.727zm3.133-4.945v1.437q-.594-.28-1.204-.43-.601-.148-1.195-.148-1.562 0-2.39 1.055-.82 1.055-.938 3.188.46-.68 1.156-1.04.696-.367 1.531-.367 1.758 0 2.774 1.07 1.023 1.063 1.023 2.899 0 1.797-1.062 2.883-1.063 1.086-2.828 1.086-2.024 0-3.094-1.547-1.07-1.555-1.07-4.5 0-2.766 1.312-4.406 1.313-1.649 3.524-1.649.593 0 1.195.117.61.118 1.266.352z\"/></g><g transform=\"translate(0, 65) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M6.25 16.66h7.5v.672L9.516 28.324H7.867l3.985-10.336H6.25z\"/></g><g transform=\"translate(375, 65) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M6.25 16.66h7.5v.672L9.516 28.324H7.867l3.985-10.336H6.25z\"/></g><g transform=\"translate(0, 20) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M10 22.785q-1.125 0-1.773.602-.641.601-.641 1.656t.64 1.656q.649.602 1.774.602t1.773-.602q.649-.61.649-1.656 0-1.055-.649-1.656-.64-.602-1.773-.602zm-1.578-.672q-1.016-.25-1.586-.945-.563-.695-.563-1.695 0-1.399.993-2.211 1-.813 2.734-.813 1.742 0 2.734.813.993.812.993 2.21 0 1-.57 1.696-.563.695-1.571.945 1.14.266 1.773 1.04.641.773.641 1.89 0 1.695-1.04 2.602-1.03.906-2.96.906t-2.969-.906Q6 26.738 6 25.043q0-1.117.64-1.89.641-.774 1.782-1.04zm-.578-2.492q0 .906.562 1.414.57.508 1.594.508 1.016 0 1.586-.508.578-.508.578-1.414 0-.906-.578-1.414-.57-.508-1.586-.508-1.023 0-1.594.508-.562.508-.562 1.414z\"/></g><g transform=\"translate(375, 20) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M10 22.785q-1.125 0-1.773.602-.641.601-.641 1.656t.64 1.656q.649.602 1.774.602t1.773-.602q.649-.61.649-1.656 0-1.055-.649-1.656-.64-.602-1.773-.602zm-1.578-.672q-1.016-.25-1.586-.945-.563-.695-.563-1.695 0-1.399.993-2.211 1-.813 2.734-.813 1.742 0 2.734.813.993.812.993 2.21 0 1-.57 1.696-.563.695-1.571.945 1.14.266 1.773 1.04.641.773.641 1.89 0 1.695-1.04 2.602-1.03.906-2.96.906t-2.969-.906Q6 26.738 6 25.043q0-1.117.64-1.89.641-.774 1.782-1.04zm-.578-2.492q0 .906.562 1.414.57.508 1.594.508 1.016 0 1.586-.508.578-.508.578-1.414 0-.906-.578-1.414-.57-.508-1.586-.508-1.023 0-1.594.508-.562.508-.562 1.414z\"/></g><rect x=\"15\" y=\"330\" width=\"45\" height=\"45\" class=\"square dark a1\" stroke=\"none\" fill=\"#d18b47\"/><rect x=\"60\" y=\"330\" width=\"45\" height=\"45\" class=\"square light b1\" stroke=\"none\" fill=\"#ffce9e\"/><rect x=\"105\" y=\"330\" width=\"45\" height=\"45\" class=\"square dark c1\" stroke=\"none\" fill=\"#d18b47\"/><rect x=\"150\" y=\"330\" width=\"45\" height=\"45\" class=\"square light d1\" stroke=\"none\" fill=\"#ffce9e\"/><rect x=\"195\" y=\"330\" width=\"45\" height=\"45\" class=\"square dark e1\" stroke=\"none\" fill=\"#d18b47\"/><rect x=\"240\" y=\"330\" width=\"45\" height=\"45\" class=\"square light f1\" stroke=\"none\" fill=\"#ffce9e\"/><rect x=\"285\" y=\"330\" width=\"45\" height=\"45\" class=\"square dark g1\" stroke=\"none\" fill=\"#d18b47\"/><rect x=\"330\" y=\"330\" width=\"45\" height=\"45\" class=\"square light h1\" stroke=\"none\" fill=\"#ffce9e\"/><rect x=\"15\" y=\"285\" width=\"45\" height=\"45\" class=\"square light a2\" stroke=\"none\" fill=\"#ffce9e\"/><rect x=\"60\" y=\"285\" width=\"45\" height=\"45\" class=\"square dark b2\" stroke=\"none\" fill=\"#d18b47\"/><rect x=\"105\" y=\"285\" width=\"45\" height=\"45\" class=\"square light c2\" stroke=\"none\" fill=\"#ffce9e\"/><rect x=\"150\" y=\"285\" width=\"45\" height=\"45\" class=\"square dark d2\" stroke=\"none\" fill=\"#d18b47\"/><rect x=\"195\" y=\"285\" width=\"45\" height=\"45\" class=\"square light e2\" stroke=\"none\" fill=\"#ffce9e\"/><rect x=\"240\" y=\"285\" width=\"45\" height=\"45\" class=\"square dark f2\" stroke=\"none\" fill=\"#d18b47\"/><rect x=\"285\" y=\"285\" width=\"45\" height=\"45\" class=\"square light g2\" stroke=\"none\" fill=\"#ffce9e\"/><rect x=\"330\" y=\"285\" width=\"45\" height=\"45\" class=\"square dark h2\" stroke=\"none\" fill=\"#d18b47\"/><rect x=\"15\" y=\"240\" width=\"45\" height=\"45\" class=\"square dark a3\" stroke=\"none\" fill=\"#d18b47\"/><rect x=\"60\" y=\"240\" width=\"45\" height=\"45\" class=\"square light b3\" stroke=\"none\" fill=\"#ffce9e\"/><rect x=\"105\" y=\"240\" width=\"45\" height=\"45\" class=\"square dark c3\" stroke=\"none\" fill=\"#d18b47\"/><rect x=\"150\" y=\"240\" width=\"45\" height=\"45\" class=\"square light d3\" stroke=\"none\" fill=\"#ffce9e\"/><rect x=\"195\" y=\"240\" width=\"45\" height=\"45\" class=\"square dark e3\" stroke=\"none\" fill=\"#d18b47\"/><rect x=\"240\" y=\"240\" width=\"45\" height=\"45\" class=\"square light f3\" stroke=\"none\" fill=\"#ffce9e\"/><rect x=\"285\" y=\"240\" width=\"45\" height=\"45\" class=\"square dark g3\" stroke=\"none\" fill=\"#d18b47\"/><rect x=\"330\" y=\"240\" width=\"45\" height=\"45\" class=\"square light h3\" stroke=\"none\" fill=\"#ffce9e\"/><rect x=\"15\" y=\"195\" width=\"45\" height=\"45\" class=\"square light a4\" stroke=\"none\" fill=\"#ffce9e\"/><rect x=\"60\" y=\"195\" width=\"45\" height=\"45\" class=\"square dark b4\" stroke=\"none\" fill=\"#d18b47\"/><rect x=\"105\" y=\"195\" width=\"45\" height=\"45\" class=\"square light c4\" stroke=\"none\" fill=\"#ffce9e\"/><rect x=\"150\" y=\"195\" width=\"45\" height=\"45\" class=\"square dark d4\" stroke=\"none\" fill=\"#d18b47\"/><rect x=\"195\" y=\"195\" width=\"45\" height=\"45\" class=\"square light e4\" stroke=\"none\" fill=\"#ffce9e\"/><rect x=\"240\" y=\"195\" width=\"45\" height=\"45\" class=\"square dark f4\" stroke=\"none\" fill=\"#d18b47\"/><rect x=\"285\" y=\"195\" width=\"45\" height=\"45\" class=\"square light g4\" stroke=\"none\" fill=\"#ffce9e\"/><rect x=\"330\" y=\"195\" width=\"45\" height=\"45\" class=\"square dark h4\" stroke=\"none\" fill=\"#d18b47\"/><rect x=\"15\" y=\"150\" width=\"45\" height=\"45\" class=\"square dark a5\" stroke=\"none\" fill=\"#d18b47\"/><rect x=\"60\" y=\"150\" width=\"45\" height=\"45\" class=\"square light b5\" stroke=\"none\" fill=\"#ffce9e\"/><rect x=\"105\" y=\"150\" width=\"45\" height=\"45\" class=\"square dark c5\" stroke=\"none\" fill=\"#d18b47\"/><rect x=\"150\" y=\"150\" width=\"45\" height=\"45\" class=\"square light d5\" stroke=\"none\" fill=\"#ffce9e\"/><rect x=\"195\" y=\"150\" width=\"45\" height=\"45\" class=\"square dark e5\" stroke=\"none\" fill=\"#d18b47\"/><rect x=\"240\" y=\"150\" width=\"45\" height=\"45\" class=\"square light f5\" stroke=\"none\" fill=\"#ffce9e\"/><rect x=\"285\" y=\"150\" width=\"45\" height=\"45\" class=\"square dark g5\" stroke=\"none\" fill=\"#d18b47\"/><rect x=\"330\" y=\"150\" width=\"45\" height=\"45\" class=\"square light h5\" stroke=\"none\" fill=\"#ffce9e\"/><rect x=\"15\" y=\"105\" width=\"45\" height=\"45\" class=\"square light a6\" stroke=\"none\" fill=\"#ffce9e\"/><rect x=\"60\" y=\"105\" width=\"45\" height=\"45\" class=\"square dark b6\" stroke=\"none\" fill=\"#d18b47\"/><rect x=\"105\" y=\"105\" width=\"45\" height=\"45\" class=\"square light c6\" stroke=\"none\" fill=\"#ffce9e\"/><rect x=\"150\" y=\"105\" width=\"45\" height=\"45\" class=\"square dark d6\" stroke=\"none\" fill=\"#d18b47\"/><rect x=\"195\" y=\"105\" width=\"45\" height=\"45\" class=\"square light e6\" stroke=\"none\" fill=\"#ffce9e\"/><rect x=\"240\" y=\"105\" width=\"45\" height=\"45\" class=\"square dark f6\" stroke=\"none\" fill=\"#d18b47\"/><rect x=\"285\" y=\"105\" width=\"45\" height=\"45\" class=\"square light g6\" stroke=\"none\" fill=\"#ffce9e\"/><rect x=\"330\" y=\"105\" width=\"45\" height=\"45\" class=\"square dark h6\" stroke=\"none\" fill=\"#d18b47\"/><rect x=\"15\" y=\"60\" width=\"45\" height=\"45\" class=\"square dark a7\" stroke=\"none\" fill=\"#d18b47\"/><rect x=\"60\" y=\"60\" width=\"45\" height=\"45\" class=\"square light b7\" stroke=\"none\" fill=\"#ffce9e\"/><rect x=\"105\" y=\"60\" width=\"45\" height=\"45\" class=\"square dark c7\" stroke=\"none\" fill=\"#d18b47\"/><rect x=\"150\" y=\"60\" width=\"45\" height=\"45\" class=\"square light d7\" stroke=\"none\" fill=\"#ffce9e\"/><rect x=\"195\" y=\"60\" width=\"45\" height=\"45\" class=\"square dark e7\" stroke=\"none\" fill=\"#d18b47\"/><rect x=\"240\" y=\"60\" width=\"45\" height=\"45\" class=\"square light f7\" stroke=\"none\" fill=\"#ffce9e\"/><rect x=\"285\" y=\"60\" width=\"45\" height=\"45\" class=\"square dark g7\" stroke=\"none\" fill=\"#d18b47\"/><rect x=\"330\" y=\"60\" width=\"45\" height=\"45\" class=\"square light h7\" stroke=\"none\" fill=\"#ffce9e\"/><rect x=\"15\" y=\"15\" width=\"45\" height=\"45\" class=\"square light a8\" stroke=\"none\" fill=\"#ffce9e\"/><rect x=\"60\" y=\"15\" width=\"45\" height=\"45\" class=\"square dark b8\" stroke=\"none\" fill=\"#d18b47\"/><rect x=\"105\" y=\"15\" width=\"45\" height=\"45\" class=\"square light c8\" stroke=\"none\" fill=\"#ffce9e\"/><rect x=\"150\" y=\"15\" width=\"45\" height=\"45\" class=\"square dark d8\" stroke=\"none\" fill=\"#d18b47\"/><rect x=\"195\" y=\"15\" width=\"45\" height=\"45\" class=\"square light e8\" stroke=\"none\" fill=\"#ffce9e\"/><rect x=\"240\" y=\"15\" width=\"45\" height=\"45\" class=\"square dark f8\" stroke=\"none\" fill=\"#d18b47\"/><rect x=\"285\" y=\"15\" width=\"45\" height=\"45\" class=\"square light g8\" stroke=\"none\" fill=\"#ffce9e\"/><rect x=\"330\" y=\"15\" width=\"45\" height=\"45\" class=\"square dark h8\" stroke=\"none\" fill=\"#d18b47\"/><use href=\"#white-bishop\" xlink:href=\"#white-bishop\" transform=\"translate(150, 330)\"/><use href=\"#white-king\" xlink:href=\"#white-king\" transform=\"translate(195, 330)\"/><use href=\"#white-knight\" xlink:href=\"#white-knight\" transform=\"translate(285, 330)\"/><use href=\"#white-rook\" xlink:href=\"#white-rook\" transform=\"translate(330, 330)\"/><use href=\"#white-pawn\" xlink:href=\"#white-pawn\" transform=\"translate(60, 285)\"/><use href=\"#white-pawn\" xlink:href=\"#white-pawn\" transform=\"translate(240, 285)\"/><use href=\"#white-pawn\" xlink:href=\"#white-pawn\" transform=\"translate(285, 285)\"/><use href=\"#white-pawn\" xlink:href=\"#white-pawn\" transform=\"translate(330, 285)\"/><use href=\"#white-queen\" xlink:href=\"#white-queen\" transform=\"translate(105, 240)\"/><use href=\"#black-king\" xlink:href=\"#black-king\" transform=\"translate(15, 195)\"/><use href=\"#white-pawn\" xlink:href=\"#white-pawn\" transform=\"translate(105, 195)\"/><use href=\"#black-pawn\" xlink:href=\"#black-pawn\" transform=\"translate(195, 150)\"/><use href=\"#white-pawn\" xlink:href=\"#white-pawn\" transform=\"translate(105, 105)\"/><use href=\"#white-rook\" xlink:href=\"#white-rook\" transform=\"translate(330, 105)\"/><use href=\"#white-pawn\" xlink:href=\"#white-pawn\" transform=\"translate(240, 60)\"/></svg>"
      ],
      "text/plain": [
       "<IPython.core.display.SVG object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import chess\n",
    "import chess.svg\n",
    "import time\n",
    "from IPython.display import SVG, display, clear_output\n",
    "\n",
    "board = chess.Board()\n",
    "\n",
    "for move in test_game_moves:\n",
    "    clear_output(wait=True)\n",
    "    board.push_san(move)\n",
    "    display(SVG(chess.svg.board(board, size=350)))\n",
    "    time.sleep(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3183fb9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "board.piece_at(chess.A4).piece_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4690754a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_square_position(square):\n",
    "    letter_ascii = ord(square[0])\n",
    "    digit = int(square[1])\n",
    "\n",
    "    return (letter_ascii - ord('a')) + (8 * (digit - 1))\n",
    "\n",
    "def encode_position(move):\n",
    "    from_position = get_square_position(move[0:2])\n",
    "    to_position = get_square_position(move[2:4])\n",
    "\n",
    "    return from_position * 63 + to_position\n",
    "\n",
    "def decode_position(encoded):\n",
    "    to_position = encoded % 63\n",
    "    from_position = encoded // 63\n",
    "\n",
    "    return (from_position, to_position)\n",
    "\n",
    "def infer_move(board, original):\n",
    "    move = board.parse_san(original)\n",
    "\n",
    "    from_square = chess.square_name(move.from_square)\n",
    "    to_square = chess.square_name(move.to_square)\n",
    "\n",
    "    return encode_position(from_square + to_square)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94fa863d",
   "metadata": {},
   "source": [
    "The idea is that we train the neural net to simply predict the next move. For that, purpose we will feed board positions, and use the actual move done by the human as label\n",
    "\n",
    "As an idea for future iterations, we can probably just use winners' moves as training data (still a player can play awfuly during the whole game, but somehow win in the end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3e05995e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n"
     ]
    }
   ],
   "source": [
    "print(chess.PAWN)\n",
    "print(chess.KNIGHT)\n",
    "print(chess.BISHOP)\n",
    "print(chess.ROOK)\n",
    "print(chess.QUEEN)\n",
    "print(chess.KING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "5918191b",
   "metadata": {},
   "outputs": [],
   "source": [
    "UNIQUE_PIECES = 6\n",
    "\n",
    "def encode_piece(piece_type, is_white):\n",
    "    if is_white:\n",
    "        return piece_type\n",
    "\n",
    "    return piece_type + UNIQUE_PIECES\n",
    "\n",
    "# form input (1-d 64 elements array) & output (1-d 4096 elements array)\n",
    "# each move is basically a separate training example\n",
    "def preprocess_game(moves):\n",
    "    board = chess.Board()\n",
    "    split_moves = moves.split(' ')\n",
    "\n",
    "    X_list = []\n",
    "    y_list = []\n",
    "\n",
    "    for move in split_moves:\n",
    "        # fill input matrix (flattened)\n",
    "        flattened_X = np.zeros(64, dtype=np.float32)\n",
    "        for square in chess.SQUARES:\n",
    "            piece = board.piece_at(square)\n",
    "            if piece:\n",
    "                flattened_X[square] = encode_piece(piece_type=piece.piece_type, is_white=piece.color)\n",
    "        \n",
    "        # fill output (encoded)\n",
    "        label = infer_move(board, move)\n",
    "\n",
    "        board.push_san(move)\n",
    "\n",
    "        X_list.append(flattened_X)\n",
    "        y_list.append(label)\n",
    "\n",
    "    return np.array(X_list), np.array(y_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "11036c3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "f30c444a",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.accelerator.current_accelerator().type if torch.accelerator.is_available() else \"cpu\"\n",
    "\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(64, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1024, 2048),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(2048, 4096),\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "\n",
    "model = NeuralNetwork().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcdddd43",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "9c94c8b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "size = len(data_df['moves'])\n",
    "\n",
    "def train(X, y, model, loss_fn, optimizer, batch):\n",
    "    model.train()\n",
    "\n",
    "    X = torch.from_numpy(X).to(device)\n",
    "    y = torch.from_numpy(y).to(device)\n",
    "\n",
    "    prediction = model(X)\n",
    "    loss = loss_fn(prediction, y)\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    if batch % 100 == 0:\n",
    "        loss, current = loss.item(), batch + 1\n",
    "        print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "d5a41848",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 4.148124  [    1/20058]\n",
      "loss: 5.888808  [  101/20058]\n",
      "loss: 5.020072  [  201/20058]\n",
      "loss: 6.008907  [  301/20058]\n",
      "loss: 6.193636  [  401/20058]\n",
      "loss: 4.439474  [  501/20058]\n",
      "loss: 6.376595  [  601/20058]\n",
      "loss: 5.479107  [  701/20058]\n",
      "loss: 6.023677  [  801/20058]\n",
      "loss: 6.271746  [  901/20058]\n",
      "loss: 3.698902  [ 1001/20058]\n",
      "loss: 5.862080  [ 1101/20058]\n",
      "loss: 6.331590  [ 1201/20058]\n",
      "loss: 5.650950  [ 1301/20058]\n",
      "loss: 5.748524  [ 1401/20058]\n",
      "loss: 5.793408  [ 1501/20058]\n",
      "loss: 5.436454  [ 1601/20058]\n",
      "loss: 4.693330  [ 1701/20058]\n",
      "loss: 4.503688  [ 1801/20058]\n",
      "loss: 5.964903  [ 1901/20058]\n",
      "loss: 6.245866  [ 2001/20058]\n",
      "loss: 5.281727  [ 2101/20058]\n",
      "loss: 5.947711  [ 2201/20058]\n",
      "loss: 5.527960  [ 2301/20058]\n",
      "loss: 3.726487  [ 2401/20058]\n",
      "loss: 5.456801  [ 2501/20058]\n",
      "loss: 5.267366  [ 2601/20058]\n",
      "loss: 5.644419  [ 2701/20058]\n",
      "loss: 5.759718  [ 2801/20058]\n",
      "loss: 5.939301  [ 2901/20058]\n",
      "loss: 5.839557  [ 3001/20058]\n",
      "loss: 5.332757  [ 3101/20058]\n",
      "loss: 3.957634  [ 3201/20058]\n",
      "loss: 5.465603  [ 3301/20058]\n",
      "loss: 5.595559  [ 3401/20058]\n",
      "loss: 5.355897  [ 3501/20058]\n",
      "loss: 5.251873  [ 3601/20058]\n",
      "loss: 6.335835  [ 3701/20058]\n",
      "loss: 5.426994  [ 3801/20058]\n",
      "loss: 6.112497  [ 3901/20058]\n",
      "loss: 5.646808  [ 4001/20058]\n",
      "loss: 5.779618  [ 4101/20058]\n",
      "loss: 3.643316  [ 4201/20058]\n",
      "loss: 5.993335  [ 4301/20058]\n",
      "loss: 5.935775  [ 4401/20058]\n",
      "loss: 5.677576  [ 4501/20058]\n",
      "loss: 5.771701  [ 4601/20058]\n",
      "loss: 4.261554  [ 4701/20058]\n",
      "loss: 5.231439  [ 4801/20058]\n",
      "loss: 4.974587  [ 4901/20058]\n",
      "loss: 6.070899  [ 5001/20058]\n",
      "loss: 4.781577  [ 5101/20058]\n",
      "loss: 6.150962  [ 5201/20058]\n",
      "loss: 5.893773  [ 5301/20058]\n",
      "loss: 5.011591  [ 5401/20058]\n",
      "loss: 4.164527  [ 5501/20058]\n",
      "loss: 5.714511  [ 5601/20058]\n",
      "loss: 5.974350  [ 5701/20058]\n",
      "loss: 5.893168  [ 5801/20058]\n",
      "loss: 3.785272  [ 5901/20058]\n",
      "loss: 5.929414  [ 6001/20058]\n",
      "loss: 4.795032  [ 6101/20058]\n",
      "loss: 5.871621  [ 6201/20058]\n",
      "loss: 5.868592  [ 6301/20058]\n",
      "loss: 5.742812  [ 6401/20058]\n",
      "loss: 5.947574  [ 6501/20058]\n",
      "loss: 5.777362  [ 6601/20058]\n",
      "loss: 5.866022  [ 6701/20058]\n",
      "loss: 6.253441  [ 6801/20058]\n",
      "loss: 4.559007  [ 6901/20058]\n",
      "loss: 6.091292  [ 7001/20058]\n",
      "loss: 4.903937  [ 7101/20058]\n",
      "loss: 5.889883  [ 7201/20058]\n",
      "loss: 5.996935  [ 7301/20058]\n",
      "loss: 4.024018  [ 7401/20058]\n",
      "loss: 6.368994  [ 7501/20058]\n",
      "loss: 4.800902  [ 7601/20058]\n",
      "loss: 5.409707  [ 7701/20058]\n",
      "loss: 6.116968  [ 7801/20058]\n",
      "loss: 5.595016  [ 7901/20058]\n",
      "loss: 5.011349  [ 8001/20058]\n",
      "loss: 4.946337  [ 8101/20058]\n",
      "loss: 5.658246  [ 8201/20058]\n",
      "loss: 6.052799  [ 8301/20058]\n",
      "loss: 6.052767  [ 8401/20058]\n",
      "loss: 6.106186  [ 8501/20058]\n",
      "loss: 4.588347  [ 8601/20058]\n",
      "loss: 5.538026  [ 8701/20058]\n",
      "loss: 5.808422  [ 8801/20058]\n",
      "loss: 4.536994  [ 8901/20058]\n",
      "loss: 5.602466  [ 9001/20058]\n",
      "loss: 5.782032  [ 9101/20058]\n",
      "loss: 6.291860  [ 9201/20058]\n",
      "loss: 4.265751  [ 9301/20058]\n",
      "loss: 5.934679  [ 9401/20058]\n",
      "loss: 4.976721  [ 9501/20058]\n",
      "loss: 5.443067  [ 9601/20058]\n",
      "loss: 5.440135  [ 9701/20058]\n",
      "loss: 5.164701  [ 9801/20058]\n",
      "loss: 5.829166  [ 9901/20058]\n",
      "loss: 5.968816  [10001/20058]\n",
      "loss: 2.951177  [10101/20058]\n",
      "loss: 4.220194  [10201/20058]\n",
      "loss: 5.762005  [10301/20058]\n",
      "loss: 4.905571  [10401/20058]\n",
      "loss: 4.433808  [10501/20058]\n",
      "loss: 6.034554  [10601/20058]\n",
      "loss: 4.627731  [10701/20058]\n",
      "loss: 5.680121  [10801/20058]\n",
      "loss: 5.589775  [10901/20058]\n",
      "loss: 5.852923  [11001/20058]\n",
      "loss: 6.544320  [11101/20058]\n",
      "loss: 5.970438  [11201/20058]\n",
      "loss: 5.691842  [11301/20058]\n",
      "loss: 5.745055  [11401/20058]\n",
      "loss: 5.891777  [11501/20058]\n",
      "loss: 5.419086  [11601/20058]\n",
      "loss: 5.004877  [11701/20058]\n",
      "loss: 5.530096  [11801/20058]\n",
      "loss: 4.539947  [11901/20058]\n",
      "loss: 5.852364  [12001/20058]\n",
      "loss: 5.275248  [12101/20058]\n",
      "loss: 5.213013  [12201/20058]\n",
      "loss: 5.843030  [12301/20058]\n",
      "loss: 5.373962  [12401/20058]\n",
      "loss: 5.327871  [12501/20058]\n",
      "loss: 3.721434  [12601/20058]\n",
      "loss: 5.911057  [12701/20058]\n",
      "loss: 4.653874  [12801/20058]\n",
      "loss: 5.472363  [12901/20058]\n",
      "loss: 5.945479  [13001/20058]\n",
      "loss: 5.183357  [13101/20058]\n",
      "loss: 5.310237  [13201/20058]\n",
      "loss: 5.340659  [13301/20058]\n",
      "loss: 5.643439  [13401/20058]\n",
      "loss: 5.465347  [13501/20058]\n",
      "loss: 4.382491  [13601/20058]\n",
      "loss: 6.013532  [13701/20058]\n",
      "loss: 5.736121  [13801/20058]\n",
      "loss: 5.763060  [13901/20058]\n",
      "loss: 6.239480  [14001/20058]\n",
      "loss: 5.608121  [14101/20058]\n",
      "loss: 5.936317  [14201/20058]\n",
      "loss: 6.071126  [14301/20058]\n",
      "loss: 5.528402  [14401/20058]\n",
      "loss: 5.228151  [14501/20058]\n",
      "loss: 6.134942  [14601/20058]\n",
      "loss: 5.901030  [14701/20058]\n",
      "loss: 4.707299  [14801/20058]\n",
      "loss: 4.600676  [14901/20058]\n",
      "loss: 5.524323  [15001/20058]\n",
      "loss: 5.913311  [15101/20058]\n",
      "loss: 4.987204  [15201/20058]\n",
      "loss: 5.135379  [15301/20058]\n",
      "loss: 5.598688  [15401/20058]\n",
      "loss: 6.281288  [15501/20058]\n",
      "loss: 4.772492  [15601/20058]\n",
      "loss: 4.744987  [15701/20058]\n",
      "loss: 4.896804  [15801/20058]\n",
      "loss: 3.731634  [15901/20058]\n",
      "loss: 5.522054  [16001/20058]\n",
      "loss: 5.408796  [16101/20058]\n",
      "loss: 5.232626  [16201/20058]\n",
      "loss: 5.325181  [16301/20058]\n",
      "loss: 5.925215  [16401/20058]\n",
      "loss: 6.090466  [16501/20058]\n",
      "loss: 4.882880  [16601/20058]\n",
      "loss: 5.341243  [16701/20058]\n",
      "loss: 5.229187  [16801/20058]\n",
      "loss: 5.545059  [16901/20058]\n",
      "loss: 5.738569  [17001/20058]\n",
      "loss: 5.424037  [17101/20058]\n",
      "loss: 5.822769  [17201/20058]\n",
      "loss: 4.717237  [17301/20058]\n",
      "loss: 4.382279  [17401/20058]\n",
      "loss: 5.967371  [17501/20058]\n",
      "loss: 5.301719  [17601/20058]\n",
      "loss: 4.814356  [17701/20058]\n",
      "loss: 5.872224  [17801/20058]\n",
      "loss: 4.481441  [17901/20058]\n",
      "loss: 4.933125  [18001/20058]\n",
      "loss: 5.379595  [18101/20058]\n",
      "loss: 4.721787  [18201/20058]\n",
      "loss: 6.620644  [18301/20058]\n",
      "loss: 6.059674  [18401/20058]\n",
      "loss: 5.367313  [18501/20058]\n",
      "loss: 4.297014  [18601/20058]\n",
      "loss: 4.534616  [18701/20058]\n",
      "loss: 5.550278  [18801/20058]\n",
      "loss: 5.642857  [18901/20058]\n",
      "loss: 5.802163  [19001/20058]\n",
      "loss: 1.850682  [19101/20058]\n",
      "loss: 5.404302  [19201/20058]\n",
      "loss: 5.670896  [19301/20058]\n",
      "loss: 7.096893  [19401/20058]\n",
      "loss: 5.218717  [19501/20058]\n",
      "loss: 5.715388  [19601/20058]\n",
      "loss: 4.287787  [19701/20058]\n",
      "loss: 6.218186  [19801/20058]\n",
      "loss: 5.117381  [19901/20058]\n",
      "loss: 4.220321  [20001/20058]\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "for batch, raw_moves in enumerate(data_df['moves']):\n",
    "    (X, y) = preprocess_game(raw_moves)\n",
    "\n",
    "    train(X, y, model, loss_fn, optimizer, batch)\n",
    "\n",
    "print(\"Done!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.13.4)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
